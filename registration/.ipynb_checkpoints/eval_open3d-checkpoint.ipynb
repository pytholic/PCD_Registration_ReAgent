{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d7e379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from prefetch_generator import BackgroundGenerator\n",
    "import easydict\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()).replace(\"/registration\", \"\"))\n",
    "import config as cfg\n",
    "cfg.BENCHMARK = True\n",
    "from environment import environment as env\n",
    "from environment import transformations as tra\n",
    "from registration.model import Agent\n",
    "import registration.model as util_model\n",
    "import utility.metrics as metrics\n",
    "from utility.visualization_open3d import CloudVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d6e26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c304f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, test_loader, dataset_name, bop_results_path=\"\", visualize=False):\n",
    "    if visualize:\n",
    "        if dataset_name != \"lm\":\n",
    "            vis = CloudVisualizer(0.01)\n",
    "        else:\n",
    "            vis = OutlineVisualizer(\"lm\", list(range(1, 16)), 0.01)\n",
    "            obj_idx = 0\n",
    "\n",
    "    agent.eval()\n",
    "    progress = tqdm(BackgroundGenerator(test_loader), total=len(test_loader))\n",
    "\n",
    "    if bop_results_path != \"\" and os.path.exists(bop_results_path):\n",
    "        open(bop_results_path, 'w').write(\"\")  # BOP toolkit expects file to contain results of a single evaluation run\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in progress:\n",
    "            source, target, pose_source, pose_target = env.init(data)\n",
    "            current_source = source\n",
    "\n",
    "            if visualize:\n",
    "                if dataset_name == \"lm\":\n",
    "                    rgb = test_loader.dataset.get_rgb(int(data['scene'][obj_idx]), int(data['frame'][obj_idx]))\n",
    "                    vis.reset(data, rgb, split=\"test\")\n",
    "                else:\n",
    "                    vis.reset(source[0, :, :3].cpu().numpy(), target[0, :, :3].cpu().numpy(),\n",
    "                              current_source[0, :, :3].cpu().numpy())\n",
    "                    vis.update(current_source[0, :, :3].cpu().numpy())\n",
    "\n",
    "            for step in range(cfg.ITER_EVAL):\n",
    "                if cfg.BENCHMARK:\n",
    "                    # re-use target embedding from first step (faster)\n",
    "                    state_emb, action_logit, state_value, target = agent(current_source, target)\n",
    "                else:\n",
    "                    state_emb, action_logit, state_value, _ = agent(current_source, target)\n",
    "                actions = util_model.action_from_logits(action_logit, deterministic=True)\n",
    "                current_source, pose_source = env.step(source, actions, pose_source, cfg.DISENTANGLED)\n",
    "\n",
    "                if visualize:\n",
    "                    if dataset_name == \"lm\":\n",
    "                        vis.update(tra.to_global(pose_source.clone(), source).cpu())\n",
    "                    else:\n",
    "                        vis.update(current_source[0, :, :3].cpu().numpy())\n",
    "            if cfg.DISENTANGLED:\n",
    "                pose_source = tra.to_global(pose_source, source)\n",
    "\n",
    "            if dataset_name == \"lm\":\n",
    "                # undo normalization\n",
    "                predictions_unnorm = pose_source.clone().cpu()\n",
    "                predictions_unnorm[:, :3, 3] *= data['normalization'][:, 0, 0][:, None]\n",
    "                # apply refinement to initial estimate to get the full model-to-camera estimation\n",
    "                #   note: prediction is from initial pose to model space\n",
    "                init_i2c = data['est']\n",
    "                prediction_m2i = torch.eye(4, device=\"cpu\").repeat(pose_source.shape[0], 1, 1)\n",
    "                prediction_m2i[:, :3, :3] = predictions_unnorm[:, :3, :3].transpose(2, 1)\n",
    "                prediction_m2i[:, :3, 3] = -(prediction_m2i[:, :3, :3] @ predictions_unnorm[:, :3, 3].view(-1, 3, 1))\\\n",
    "                    .squeeze()\n",
    "                estimates_m2c = init_i2c @ prediction_m2i\n",
    "                # save in BOP format\n",
    "                estimates_bop = \"\"\n",
    "                for i_est, estimate in enumerate(estimates_m2c):\n",
    "                    scene_id, im_id, obj_id = data['scene'][i_est], data['frame'][i_est],\\\n",
    "                                              data['gt']['obj_id'][i_est]\n",
    "                    conf, duration = 1.0, 0.0  # no confidence estimated, duration would need to be for single frame\n",
    "                    estimates_bop += f\"{scene_id},{im_id},{obj_id},{conf:0.3f},\" \\\n",
    "                                     f\"{' '.join([f'{float(v):0.6f}' for v in estimate[:3, :3].reshape(-1)])},\" \\\n",
    "                                     f\"{' '.join([f'{float(v):0.6f}' for v in estimate[:3, 3].reshape(-1)])},\" \\\n",
    "                                     f\"{duration:0.3f}\\n\"\n",
    "                with open(bop_results_path, 'a') as file:\n",
    "                    file.write(estimates_bop)\n",
    "            else:\n",
    "                predictions.append(pose_source)\n",
    "            #print(pose_source)\n",
    "\n",
    "    # COMPUTE STATISTICS\n",
    "    if dataset_name == \"lm\":\n",
    "        print(f\"Stored predictions in BOP format to {bop_results_path}.\")\n",
    "    else:\n",
    "        predictions = torch.cat(predictions) # contains transformation matrix\n",
    "        print(predictions)\n",
    "        eval_metrics, summary_metrics = metrics.compute_stats(predictions, data_loader=test_loader)\n",
    "\n",
    "        # log test metrics\n",
    "        print(f\"MAE R: {summary_metrics['r_mae']:0.2f}\")\n",
    "        print(f\"MAE t: {summary_metrics['t_mae']:0.3f}\")\n",
    "        print(f\"ISO R: {summary_metrics['r_iso']:0.2f}\")\n",
    "        print(f\"ISO t: {summary_metrics['t_iso']:0.3f}\")\n",
    "        print(f\"ADI AUC: {(summary_metrics['adi_auc10'] * 100):0.1f}%\")\n",
    "        print(f\"CD: {summary_metrics['chamfer_dist'] * 1000:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46603459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loading weights...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:02<00:00, 62.28s/it]\n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4832, -0.8641,  0.1407,  0.1295],\n",
      "         [ 0.3661,  0.3453,  0.8641, -0.0693],\n",
      "         [-0.7953, -0.3661,  0.4832, -0.0890],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0')\n",
      "MAE R: 53.86\n",
      "MAE t: 0.070\n",
      "ISO R: 104.67\n",
      "ISO t: 0.123\n",
      "ADI AUC: 10.0%\n",
      "CD: 5.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    args = easydict.EasyDict({'mode': 'ilrl',\n",
    "                             'dataset': 'Custom',\n",
    "                             'batch_size': 1,\n",
    "                             'visualize': True})\n",
    "\n",
    "    code_path = os.path.dirname(os.getcwd()).replace(\"/registration\", \"\")\n",
    "    if args.dataset.startswith(\"Custom\"):\n",
    "        from dataset.dataset_custom import CustomDataset\n",
    "        test_dataset = CustomDataset(\"test\", \"jitter\")\n",
    "        pretrain = os.path.join(code_path, f\"weights/Custom_{args.mode}.zip\")  # same weights for M40 and SON\n",
    "        bop_results_path = \"\"\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    print(\"  loading weights...\")\n",
    "    agent = Agent().to(DEVICE)\n",
    "    if os.path.exists(pretrain):\n",
    "        util_model.load(agent, pretrain)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No weights found at {pretrain}. Download pretrained weights or run training first.\")\n",
    "\n",
    "    evaluate(agent, test_loader, args.dataset, bop_results_path, args.visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5ac64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
